{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Elasticsearch Integration\n",
    "This notebook demonstrates how you can use Spark to read in streaming data, perform a transform on the data, and write the new data to Elasticsearch.\n",
    "\n",
    "It also demonstrates how you can use Spark to read in an index from Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing ES-Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the Elastic-hadoop connector. This should be installed on the same path across your entire cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the ES-Hadoop connector is not installed, install it on the driver. Note again that this should be installed across the entire cluster if running this in production. Since this example uses a single node cluster we are in good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'elasticsearch-hadoop-6.1.1' not in os.listdir():\n",
    "    es_hadoop = urllib.request.URLopener()\n",
    "    es_hadoop.retrieve(\"http://download.elastic.co/hadoop/elasticsearch-hadoop-6.1.1.zip\", \"es-hadoop.zip\")\n",
    "\n",
    "    with zipfile.ZipFile(\"es-hadoop.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's install the `elasticsearch` Python library on this Spark node so we can set up an Elasticsearch index quickly and easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Elasticsearch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and create an index called `stream-test`. If it already exists, let's wipe it out and start over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first set up Elasticsearch connection\n",
    "# by default we connect to elasticsearch:9200 \n",
    "# since we are running this notebook from the Spark-Node we need to use `elasticsearch` instead of `localhost`\n",
    "# as this is the name of the docker container running Elasticsearch\n",
    "es = Elasticsearch('elasticsearch:9200')\n",
    "\n",
    "# if the stream-test index exists, wipe it out and create a new one\n",
    "if es.indices.exists('stream-test'):\n",
    "    es.indices.delete('stream-test')\n",
    "    es.indices.create('stream-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure that the ES-Hadoop connector is on the driver's classpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-class-path=elasticsearch-hadoop-6.1.1/dist/elasticsearch-spark-20_2.11-6.1.1.jar pyspark-shell'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonSparkStreaming\")  \n",
    "sc.setLogLevel(\"WARN\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and our Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note that the second argument is the batch time\n",
    "ssc = StreamingContext(sc, 3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's generate a file stream. In this case we are going to stream in all files written to the `sample` directory. This is being pumped full of random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stream = ssc.textFileStream('sample/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to want to perform some slight transforms on this data. Primarily we want to parse the epoch time into a date string that Elasticsearch can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_sample(x):\n",
    "    data = json.loads(x)\n",
    "    data['timestamp'] = datetime.fromtimestamp(data['timestamp']).strftime('%Y/%m/%d %H:%M:%S')\n",
    "    data['doc_id'] = data.pop('count')\n",
    "    return (data['doc_id'], json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will map the function we just created to the stream so that each record, in each batch, is parsed with the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = stream.map(lambda x: format_sample(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function that writes the RDD generated by each streaming batch operation to Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handler(rdd):\n",
    "        es_write_conf = {\n",
    "        # specify the node that we are sending data to (this should be the master)\n",
    "        \"es.nodes\" : 'elasticsearch',\n",
    "            \n",
    "        # specify the port in case it is not the default port\n",
    "        \"es.port\" : '9200',\n",
    "            \n",
    "        # specify a resource in the form 'index/doc-type'\n",
    "        \"es.resource\" : 'stream-test/sample',\n",
    "\n",
    "        # is the input JSON?\n",
    "        \"es.input.json\" : \"yes\",\n",
    "            \n",
    "        # is there a field in the mapping that should be used to specify the ES document ID\n",
    "        \"es.mapping.id\": \"doc_id\",\n",
    "        }\n",
    "\n",
    "        rdd.saveAsNewAPIHadoopFile(\n",
    "                path='-',\n",
    "                outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "                keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "                valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "\n",
    "                # critically, we must specify our `es_write_conf` \n",
    "                conf=es_write_conf)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply our handler to each record streaming through the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed.foreachRDD(lambda rdd: handler(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can request it is printed to stdout as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start the spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to stop the context we need to invoke the stop method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Processing ES with Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a new Spark Context since we killed our last one. We'll use this to operate on an entire index of ES data using Spark. In this case we'll read back in data we just shipped using Spark Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonSparkReading\")  \n",
    "sc.setLogLevel(\"WARN\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_read_conf = { \n",
    "    # specify the node that we are sending data to (this should be the master)    \n",
    "    \"es.nodes\" : \"elasticsearch\",\n",
    "    \n",
    "    # specify the read resource in the format 'index/doc-type'\n",
    "    \"es.resource\" : \"stream-test/sample\"\n",
    "    }\n",
    "\n",
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take in a sample of the data we just pulled from Elasticsearch. Note that each object is a tuple where the first item is the document id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert these tuples to pure JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_rdd = es_rdd.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert the RDD into a Spark SQL Dataframe so we can treat it more like a Pandas object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "\n",
    "# Executes a monkey patch to the Spark Context, extending it with Spark SQL capabilities\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = es_rdd.map(lambda l: Row(**dict(l))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute a groupby on the data. In this case we'll groupby `name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .groupby('name') \\\n",
    "    .count() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter data as we would in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .filter(df.name == 'Samwise')\\\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to stop the context if we wish to switch back to a Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
